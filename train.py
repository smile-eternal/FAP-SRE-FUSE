import datasetimport modelimport configimport torchimport numpy as npimport osimport cv2from tensorboardX import SummaryWriterimport torch.nn.functional as Fimport randomimport timeimport torch.optim.lr_scheduler as lr_sfrom torchvision.utils import save_imageimport skimage.measure as skfrom collections import OrderedDictfrom torch.autograd import Variableimport argparseimport yamlfrom torch.backends import cudnnimport torchvisiondef gradient(x):    l=x    r=F.pad(x,[0,1,0,0])[:,:,:,1:]    t=x    b=F.pad(x,[0,0,0,1])[:,:,1:,:]    dx,dy=torch.abs(r-l),torch.abs(b-t)    dx[:,:,:,-1]=0    dy[:,:,-1,:]=0    return dx,dydef Totensor(x):    x=np.transpose(x,(2,0,1))    x=torch.from_numpy(x).float()/255.0    return xif __name__=='__main__':    #load yaml    parser = argparse.ArgumentParser()    parser.add_argument("--config_path",                        type=str,                        default="./checkpoint_x2/FAP_SRE_Fuse_x2_Lytro.yaml",                        required=True,                        help="Path to test config file.")    args = parser.parse_args()    with open(args.config_path, "r") as f:        config = yaml.full_load(f)    # Fixed random number seed    random.seed(config["SEED"])    np.random.seed(config["SEED"])    torch.manual_seed(config["SEED"])    torch.cuda.manual_seed_all(config["SEED"])    torch.cuda.empty_cache()    # Because the size of the input image is fixed, the fixed CUDNN convolution method can greatly increase the running speed    cudnn.benchmark = True    use_gpu=torch.cuda.is_available()    writer=SummaryWriter()    attention_module=getattr(model,config["MODEL"]["G"]["Attention"])()    attention_module.load_state_dict(torch.load(config["MODEL"]["G"]["ATTENTION_WEIGHTS_PATH"]))    attention_module.eval()    gather_model=getattr(model,config["MODEL"]["G"]["Gather"])(config,attention_module)    if(use_gpu):        gather_model=gather_model.cuda()    data_load=dataset.data_loader(config)    optimizer=torch.optim.Adam(gather_model.parameters(),lr=1e-3)    schedular=lr_s.StepLR(optimizer,step_size=800,gamma=0.5)    crizer=model.L1_loss()    if not os.path.exists(config["MODEL"]["G"]["SAVE_DIR"]):        os.mkdir(config["MODEL"]["G"]["SAVE_DIR"])    for epoch in range(config["MODEL"]["G"]["EPOCH"]):        print("epoch:{}".format(epoch))        for i,data in enumerate(data_load,0):            train_lr1=data['input1']            train_lr2=data['input2']            train_lr3=data['input3']            label=data['labels']            at_train1=data['at_train']            at_train2=data['at_train2']            map=data['maps']            map2=data['map2']            optimizer.zero_grad()            if (use_gpu):                train_lr1,train_lr2,train_lr3,label,at_train1,at_train2,map,map2=train_lr1.cuda(),train_lr2.cuda(),train_lr3.cuda(),label.cuda(),at_train1.cuda(),at_train2.cuda(),map.cuda(),map2.cuda()            output1,output2,out,x_mask,y_mask=gather_model(train_lr1,train_lr2)            loss_rdn1=crizer(output1,label)            loss_rdn2=crizer(output2,label)            g1,g2=gradient(output1)            g3,g4=gradient(label)            g5,g6=gradient(output2)            g7,g8=gradient(label)            loss_g1=crizer(g1,g3)+crizer(g2,g4)            loss_g2=crizer(g5,g7)+crizer(g6,g8)            loss_m1=crizer(Variable(out*x_mask,requires_grad=True),Variable(at_train1*x_mask,requires_grad=True))            loss_m2=crizer(Variable(out*y_mask,requires_grad=True),Variable(at_train2*y_mask,requires_grad=True))            loss_gather=crizer(out,label)            if epoch <= 500:                loss = loss_rdn1+loss_rdn2+loss_g2+loss_g1            if epoch > 500:                loss = loss_gather+loss_m1+loss_m2            if(use_gpu):                loss=loss.cuda()            loss.backward()            optimizer.step()            schedular.step()            torch.save(gather_model.state_dict(),config["MODEL"]["G"]["SAVE_DIR"]+'/net_parameters{}'.format(epoch))            print('epoch:{}ï¼Œloss:{}'.format(epoch,loss))            writer.add_scalar('loss',loss,epoch)        # if  epoch % 10 == 0:        #     print(gather_model.attention.bn.state_dict())        #     print(gather_model.reconstruct.state_dict())        #     save_image(out,'x01.png')        #     print(out.size())        #     save_image(output1, 'x02.png')        #     save_image(output2, 'x03.png')        #     save_image(train_lr1,'x04.png')        #     save_image(train_lr2,'x05.png')        #     # save_image(x_mask, 'x04.png')        #     # save_image(y_mask, 'x05.png')        #     save_image(label,'x06.png')        #     # save_image(at_train1,'x07.png')        #     # save_image(at_train2,'x08.png')